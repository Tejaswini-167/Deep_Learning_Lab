{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnrZ7nmfSAHy"
      },
      "source": [
        "### Lab 30: Attention Model for Image Captioning\n",
        "\n",
        "Objective: Implement an image captioning system with Bahdanau attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8J4wY9aRf-n"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install tensorflow pillow tqdm --quiet\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------\n",
        "# 1. Dataset Setup\n",
        "# -----------------\n",
        "# NOTE: In real usage, download Flickr8k dataset and extract to ./Flickr8k/\n",
        "# Here we simulate with a small set due to time/memory limits.\n",
        "\n",
        "# For demonstration, create a dummy mapping of image -> captions\n",
        "image_paths = [\"sample1.jpg\", \"sample2.jpg\"]  # Replace with actual paths\n",
        "captions = {\n",
        "    \"sample1.jpg\": [\"a dog running in the grass\", \"a brown dog is playing outdoors\"],\n",
        "    \"sample2.jpg\": [\"a man riding a bicycle\", \"a cyclist on a road\"]\n",
        "}\n",
        "\n",
        "# -----------------\n",
        "# 2. Image Preprocessing\n",
        "# -----------------\n",
        "def load_image(path):\n",
        "    img = Image.open(path).resize((299, 299))  # InceptionV3 size\n",
        "    img = np.array(img) / 255.0\n",
        "    return img\n",
        "\n",
        "# -----------------\n",
        "# 3. Tokenizer Setup\n",
        "# -----------------\n",
        "all_captions = []\n",
        "for cap_list in captions.values():\n",
        "    all_captions.extend(cap_list)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = max(len(c.split()) for c in all_captions)\n",
        "\n",
        "# -----------------\n",
        "# 4. CNN Encoder (Pre-trained)\n",
        "# -----------------\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "encoder_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.fc = layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "# -----------------\n",
        "# 5. Bahdanau Attention\n",
        "# -----------------\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = layers.Dense(units)\n",
        "        self.W2 = layers.Dense(units)\n",
        "        self.V = layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# -----------------\n",
        "# 6. RNN Decoder with Attention\n",
        "# -----------------\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = layers.GRU(self.units,\n",
        "                              return_sequences=True,\n",
        "                              return_state=True,\n",
        "                              recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = layers.Dense(self.units)\n",
        "        self.fc2 = layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.fc2(x)\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))\n",
        "\n",
        "# -----------------\n",
        "# 7. Instantiate Encoder-Decoder\n",
        "# -----------------\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "# -----------------\n",
        "# 8. Dummy Feature Extraction\n",
        "# -----------------\n",
        "# Normally, extract features for each image\n",
        "dummy_img = np.random.rand(1, 8, 8, 2048).astype(np.float32)  # Simulated InceptionV3 output\n",
        "features = encoder(dummy_img)\n",
        "\n",
        "# -----------------\n",
        "# 9. Dummy Caption Generation\n",
        "# -----------------\n",
        "hidden = decoder.reset_state(batch_size=1)\n",
        "dec_input = tf.expand_dims([tokenizer.word_index['<unk>']], 0)  # Start token\n",
        "result = []\n",
        "for i in range(max_length):\n",
        "    predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    result.append(predicted_id)\n",
        "    if predicted_id == 0:  # Suppose 0 is <end>\n",
        "        break\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "print(\"Generated Token IDs:\", result)\n",
        "\n",
        "# -----------------\n",
        "# 10. Visualization of Attention (Dummy)\n",
        "# -----------------\n",
        "# Skipped actual visualization since we used dummy data\n",
        "print(\"Attention model setup complete. Ready for real dataset training.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
