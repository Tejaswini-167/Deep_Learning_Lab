## Lab 5: Backpropagation from Scratch (NumPy)

### Objective
Implement a simple neural network to solve the XOR problem using backpropagation coded manually with NumPy.

### Key Concepts
- XOR dataset
- Feedforward pass & sigmoid activation
- MSE loss function
- Backpropagation (manual gradients)
- Gradient descent updates

### Implementation
- Built a 2-2-1 neural network with sigmoid activations.
- Computed forward pass, loss, and gradients manually.
- Trained for 5000 epochs and plotted loss using Matplotlib.
- Evaluated predictions on XOR inputs.

### Results / Observations
- Loss decreased steadily across epochs.
- Final predictions matched true XOR outputs (0, 1, 1, 0).
- Demonstrated the fundamentals of training without high-level libraries.
